---
title: "Introducing sparklyr"
output:
  html_document:
    df_print: paged
---

## Setup

- Install updated versions of `dplyr` and `sparklyr`

```{r, eval = FALSE}
  install.packages("sparklyr")
  install.packages("dplyr")
```

- Load the libraries 

```{r}
  library(sparklyr)
  library(dplyr)
  library(tidyverse)
```

- Install Spark version 2.1.0 locally in your computer
```{r}
  spark_install(version = "2.1.0")
```

## Create a Spark session

We will use a custom configuration for the `sparklyr` connection, we are requesting:

- 16 gigabytes of memory for the `driver`
- Make 80% of the memory accessible to use during the analysis

```{r}
  conf <- spark_config()
  conf$`sparklyr.shell.driver-memory` <- "7G"  
  conf$spark.memory.fraction <- 0.8 
```

- Make sure to pass the `conf` variable as the value for the `config` argument
- Navigate to http://127.0.0.1:4040/executors/
- In the **Executors** section there is 5.1 GB of Storage Memory assigned (7 * 80%)
- There are also 8 cores assigned

```{r}
  sc <- spark_connect(master = "local", config = conf, version = "2.1.0")
```

## Copy data into Spark

## File Setup

To ensure reproducibility, this chunk downloads and save the needed files into the `data` folder.  The folder is created if it does not exist in your Workspace.

```{r}
library(nycflights13)
flights <- as_data_frame(flights)
write_csv(flights, "./flights.csv", na = "NA")
```

## Load data 

This next line does the following:

- Creates a new table in the Spark environment called `flights`
- Points Spark to the `data` folder as its source
- Asks that the data is not brought into Spark memory
- Supplies the column names and tells Spark not to try to figure out the schema

```{r}
sp_flights <- spark_read_csv(sc, 
                             name = "flights", 
                             path = "./flights.csv", 
                             memory = FALSE)
sp_flights_memory <- spark_read_csv(sc, 
                             name = "flights_mem", 
                             path = "./flights.csv", 
                             memory = TRUE)
```

- See http://127.0.0.1:4040/storage/, to confirm that there's nothing in Storage, yet

## Spark SQL

- Use the `DBI` package for SQL operations in `sparklyr`
- `dbGetQuery()` pulls the data into R automatically

```{r}
  library(DBI)
  
  top10 <- dbGetQuery(sc, "Select * from flights limit 10")
  
  top10
```

### Use SQL in a code chunk

- RMarkdown allows non-R chunks like SQL: http://rmarkdown.rstudio.com/authoring_knitr_engines.html#sql 

```{sql, connection = sc}
  SELECT  * FROM flights WHERE origin = "EWR" LIMIT 10
```

### dplyr

- Use `dplyr` verbs to interact with the data

```{r}
flights_table <- sp_flights %>%
  mutate(dep_delay = as.numeric(dep_delay),
         arr_delay = as.numeric(arr_delay),
         sched_dep_time = as.numeric(sched_dep_time)) %>%
  select(origin, dest, sched_dep_time, sched_arr_time, arr_delay, dep_delay, month) 
#%>%  show_query()
flights_table %>% head
```

### Store Data 

Use a spark write function to store the data in HDFS or s3.

```{r}
  spark_write_csv(flights_table, "tmp.csv")
  spark_write_parquet(flights_table, "tmp.parquet")
  spark_write_json(flights_table, "tmp.json")
```

### show_query()

- Use `show_query()` to display what is the SQL query that `dplyr` will send to Spark

```{r}
  sp_flights  %>% 
    head %>% 
    show_query()
```

### Compute

-`compute()`  caches a Spark DataFrame into memory
- It performs these two operations: `sdf_register()` + `tbl_cache()`
- After the code below completes, see http://127.0.0.1:4040/storage/, the is a new table called `flights_subset`

```{r}
  subset_table <- flights_table %>% 
    compute("flights_subset")
```

- Now we can perform more complex aggregations

```{r}
subset_table %>%
  group_by(dest) %>%
  summarise(n = n(),
            mean_sdt = mean(sched_dep_time),
            mean_sat = mean(sched_arr_time))
```

## Improved sampling

```{r}
# Improved in sparklyr 0.6
  sp_flights %>% 
    sample_frac(0.0001) %>% 
    show_query()
```

```{r}
  sp_flights %>% 
    sample_frac(0.0001) %>% 
    group_by(Year) %>%
    tally
```

## Spark DataFrame (sdf) Functions

### sdf_pivot() 

New in `sparklyr` 0.6! - Construct a pivot table over a Spark Dataframe, using a syntax similar to that from `reshape2::dcast()` and `tidyr::spread`

```{r}
subset_table %>%
  group_by(origin, dest) %>%
  tally() %>%
  head()
```

```{r}
subset_table %>%
  filter(origin == "EWR" | origin == "JFK") %>%
  sdf_pivot(origin~dest) 
```

## Feature Transformers (ft) 

https://spark.apache.org/docs/latest/ml-features.html

### ft_binarizer()

- Apply threshold to a column, such that values less than or equal to the threshold are assigned the value 0.0, and values greater than the threshold are assigned the value 1.0.
- [*The Federal Aviation Administration (FAA) considers a flight to be delayed when it is 15 minutes later than its scheduled time.*](https://en.wikipedia.org/wiki/Flight_cancellation_and_delay)

```{r}
subset_table %>%
  ft_binarizer(input.col =  "dep_delay", 
               output.col = "delayed",
               threshold = 15) %>%
  head(200)
```

### ft_bucketizer()

- Similar to R's `cut()` function, this transforms a numeric column into a discretized column, with breaks specified through the splits parameter.

```{r}
subset_table %>%
  ft_bucketizer(input.col =  "sched_dep_time",
                output.col = "DepHour",
                splits = c(0, 400, 800, 1200, 1600, 2000, 2400)) %>%
  head(100)
```

## MLib

- `sparklyr` enables us to use `dplyr` verbs, `sdf` functions and `ft` functions to prepare data within a single piped code segment

```{r}
sample_data <- subset_table %>%
  filter(!is.na(dep_delay)) %>%
  ft_binarizer(input.col = "dep_delay",
               output.col = "delayed",
               threshold = 30) %>% 
  ft_bucketizer(input.col =  "sched_dep_time",
                output.col = "DepHour",
                splits = c(0, 400, 800, 1200, 1600, 2000, 2400)) %>%
  mutate(DepHour = paste0("h", as.integer(DepHour))) %>%
  sdf_partition(training = 0.001, testing = 0.009, other = 0.99)
```

```{r}
  training <- compute(sample_data$training, "training")
```

- A formula can be used for modeling, as in: `x ~ y + z` 

```{r}
delayed_model <- ml_logistic_regression(training, delayed ~ dep_delay + DepHour) 
summary(delayed_model)
```

- We will use the `testing` sample to run predictions
- It returns the same Spark DataFrame but with new columns

```{r}
delayed_testing <- sdf_predict(delayed_model, sample_data$testing) 
delayed_testing %>% head
```

- Let's see how the model performed

```{r}
delayed_testing %>%
  group_by(delayed, prediction) %>%
  tally 
```

## Distributed R

`spark_apply()` applies an R function to a Spark object.  The R function runs over each RDD in Spark. Please read this article: https://spark.rstudio.com/articles/guides-distributed-r.html

- The `training` Spark DataFrame has 8 partitions, `nrow()` will run in each partition 

```{r}
  training %>%
    spark_apply(nrow)
```

### Group by

- The `group_by` argument can be used to run the R function over a specific column or columns instead of the RDD partitions.

```{r}
training %>%
  spark_apply(nrow, group_by =  "month", columns = "count")
```

## Distributing Packages

- With spark_apply() you can use any R package inside Spark. For instance, you can use the broom package to create a tidy data from a `glm()` model output.

```{r}
spark_apply(
  training,
  function(e) broom::tidy(glm(delayed ~ arr_delay, data = e, family = "binomial")),
  names = c("term", "estimate", "std.error", "statistic", "p.value"),
  group_by = "origin")
```

```{r}
# plotting package
# httr::set_config( httr::config( ssl_verifypeer = 0L ) )
# devtools::install_github("edgararuiz/dbplot")
library(dbplot)

subset_table %>% 
  dbplot_histogram(sched_dep_time)

subset_table %>%
  filter(!is.na(arr_delay)) %>%
  dbplot_raster(arr_delay, dep_delay) 

subset_table %>%
  dbplot_bar(origin)

subset_table %>%
  dbplot_line(month)

subset_table %>%
  dbplot_boxplot(origin, dep_delay)

```
